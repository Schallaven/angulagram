#!/usr/bin/env python2
# -*- coding: utf-8 -*-

# Copyright (C) 2019 by Sven Kochmann

# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the  Free Software Foundation,  either version 3  of the License, or
# (at your option) any later version.

# This program  is distributed  in the hope  that it will  be  useful,
# but  WITHOUT  ANY  WARRANTY;  without even  the implied warranty  of
# MERCHANTABILITY  or  FITNESS  FOR  A  PARTICULAR  PURPOSE.  See  the
# GNU General Public License for more details.

# You  should  have  received a copy of the GNU General Public License
# along with this program. If not, see <http://www.gnu.org/licenses/>.

# This script evaluates  an angulagram  using the strategies explained
# in  DOI: 10.1021/acs.analchem.8b02186  and the given parameters. The
# name 'evolutin' was generated by chem-name-gen
# (DOI: 10.5281/zenodo.2578428).

import os
import csv
import numpy
import scipy.signal
import scipy.interpolate
import scipy.optimize
import matplotlib.pyplot as plt
import argparse

# Argument setup and parsing to a dictionary
parser = argparse.ArgumentParser(description='Evaluates an angulagram using strategies explained in '
                                             'Kochmann et al., Anal. Chem. 90, 9504-9509 (2018). DOI: '
                                             '10.1021/acs.analchem.8b02186')

parser.add_argument('-v', '--version', help='prints version information', action='version',
                    version='evolutin 1.0 by Sven Kochmann')

parser.add_argument('infile', action='store', nargs='?', type=str)

parser.add_argument('-s', '--skip', action='store', type=int, default=0, metavar='N',
                    help='Skips the first N lines when reading infile.')
parser.add_argument('-b', '--background', action='store', type=int, default=10, metavar='N',
                    help='The first N values of the data sets are considered background.')
parser.add_argument('-d', '--digits', action='store', type=int, default=2, metavar='N',
                    help='Results are formatted to have N digits (after period).')
parser.add_argument('-o', '--order', action='store', type=int, default=5, metavar='N',
                    help='Uses N points to compare for finding max/min values.')
parser.add_argument('-f', '--filter', action='store', type=int, default=[0, 0], metavar=('W', 'P'), nargs=2,
                    help='Applies the Savitzky-Golay filter to smooth the data before evaluation. W is the '
                         'window size and P the polynomial order. If W = 0 (default) no filter is applied.')

args = vars(parser.parse_args())

# Read in and store the data values; remove first N lines of data
data = []
with open(args['infile']) as inputfile:
    for _ in range(args['skip']):
        next(inputfile)
    reader = csv.reader(inputfile, delimiter='\t', quoting=csv.QUOTE_NONNUMERIC)
    data = list(reader)

data = numpy.array(data)

if args['filter'][0] > 0 and args['filter'][1] > 0:
    data[:, 1] = scipy.signal.savgol_filter(data[:, 1], args['filter'][0], args['filter'][1])

# Evolutin will evaluate the angulagram using the following steps:
# 0. Find background (first N signals)
# 1. Find extreme (max/min) values; max values -> stream deflection
# 2. Find the two border values
# 3. Create a list sections
# 4. Evaluate each section:
# 4.1 find width of peak or section -> stream width
# 4.2 fit Lorentz function to section -> stream linearity
deflections = []
widths = []
linearities = []

background = numpy.average(data[0:args['background'], 1])

# These are the max/min value-indices! For φ-values, use data[:, 0][max] and for signal values
# use data[:, 1][max]
max = scipy.signal.argrelmax(data[:, 1], order=args['order'])[0]
min = scipy.signal.argrelmin(data[:, 1], order=args['order'])[0]
max = numpy.delete(max, numpy.where(data[:, 1][max] < background))
min = numpy.delete(min, numpy.where(data[:, 1][min] < background))

# There should be exactly N maxima and N-1 minima!
if (len(max)-1) is not len(min):
    print("Found %d maxima and %d minima. There should be exactly N maxima and N-1 minima! Try changing 'order'." %
          (len(max), len(min)))
    exit(-1)

# These are the two border values for the outer peaks. Otherwise, the fitting later would take
# lots of background into account; check also that the border is above the background!
leftidx = (numpy.abs(data[:, 1][0:max[0]] - (0.1 * data[:, 1][max][0]))).argmin()
rightidx = (numpy.abs(data[:, 1][max[-1]:] - (0.1 * data[:, 1][max][-1]))).argmin()

backgroundleft = (numpy.abs(data[:, 1][0:max[0]] - background)).argmin()
backgroundright = (numpy.abs(data[:, 1][max[-1]:] - background)).argmin()
if leftidx < backgroundleft:
    leftidx = backgroundleft
if rightidx > backgroundright:
    rightidx = backgroundright

# Concatenated list of sections (border values + minima):
sectionindices = sorted(min.tolist() + [leftidx, rightidx + max[-1]])
sections = []
for i in xrange(len(sectionindices) - 1):
    # save as tuple: (max, left border, right border), all indices!
    sections.append((max[i], sectionindices[i], sectionindices[i+1]))

# Evalute each section here:
# 1. find width at FWHM of section maximum: initially, just take the border φ-values for calculating the width.
#    however, if the border signals are less than FWHM, then try to find the points at 0.5 * max signal in section
#    by interpolation of points.
# 2. find linearity by calculating the R-square between measured data and an ideal stream function
def Lorentzian(xdata, height, width, pos, offset):
    return height * (width/2)**2 / ((width/2)**2 + (pos - xdata)**2) + offset

FWHMpoints = []
fitparams = []
for sec in sections:
    sectionangles = data[sec[1]:(sec[2]+1), 0]
    sectionsignals = data[sec[1]:(sec[2]+1), 1]
    FWHM = data[:, 1][sec[0]] * 0.5

    # Maxima and minima have to fit: Max1 Min1 Max2 Min2 Max3, if they don't it means that we found minima and maxima
    # that aren't really ones. Tell user to increase 'order' parameter
    if (sec[0] - sec[1]) > len(sectionangles):
        print("Found maxima and minima that are not in the right order. Try increasing order parameter "
              "(currently: %d)." % (args['order']))
        print("Maxima indices:", max)
        print("Minima indices:", min)
        exit(-1)

    # FWHM calculations
    leftpt = (data[:, 0][sec[1]], FWHM)
    rightpt = (data[:, 0][sec[2]], FWHM)

    try:
        polf = scipy.interpolate.UnivariateSpline(sectionangles, sectionsignals - FWHM, s=0)
        roots = polf.roots()

        if len(roots) == 2:
            leftpt = (roots[0], FWHM)
            rightpt = (roots[1], FWHM)
        elif len(roots) == 1 and (roots[0] - data[:, 0][sec[0]]) > 0:
            rightpt = (roots[0], FWHM)
        elif len(roots) == 1 and (roots[0] - data[:, 0][sec[0]]) < 0:
            leftpt = (roots[0], FWHM)
    except:
        pass

    FWHMpoints = FWHMpoints + [leftpt, rightpt]
    fullwidth = rightpt[0] - leftpt[0]

    # Linearity calculations
    parameterguess = [sectionsignals.max() - background, fullwidth, sectionangles[sec[0] - sec[1]], background]

    # try optimization first: this makes sure that small discrepancies in the measured data (e.g. that the maximum
    # is shifted slightly from the actual peak point reducing the linearity) due to resolution (1° resolution in φ
    # gives edged peaks) or other effects (non-ideal peak shape due to non-ideal readout of reflectometric imaging
    # data). optimization will not cover up "bad linearity": excessively bended streams still result in low L²
    try:
        popt, pcov = scipy.optimize.curve_fit(Lorentzian, sectionangles, sectionsignals, p0=parameterguess)
        parameterguess = popt
    except:
        pass

    # Calculate r-squared to be used as linearity value
    ss_res = numpy.sum((sectionsignals - Lorentzian(sectionangles, *parameterguess)) ** 2)
    ss_tot = numpy.sum((sectionsignals - numpy.mean(sectionsignals)) ** 2)
    r_squared = 1 - (ss_res / ss_tot)
    deflections.append(parameterguess[2])
    widths.append(parameterguess[1])
    linearities.append(r_squared)
    fitparams.append(parameterguess)

# Create output file
output = list(zip(range(1, len(deflections)+1), deflections, widths, linearities))

resolutions = []
for l, left in enumerate(output):
    for r, right in enumerate(output[(l+1):]):
        resolutions.append((l+1, r+l+2, (right[1]-left[1])/(0.5*(left[2]+right[2]))))

floatprecision = "%." + str(args['digits']) + "f"
with open(os.path.splitext(args['infile'])[0]+'.evaluated.txt', 'w') as f:
    f.write('Stream\tDeflection (°)\tFWHM (°)\tLinearity\r\n')
    for line in output:
        f.write(('%d' + ('\t' + floatprecision)*3 + '\r\n') % line)
    f.write('\r\n')
    f.write('Stream 1\tStream 2\tResolution\r\n')
    for line in resolutions:
        f.write(('%d\t%d\t' + floatprecision + '\r\n') % line)
    f.write('\r\n')
    f.write('Number of background points:\t%d\r\n' % (args['background']))
    f.write(('Background signal:\t' + floatprecision + '\r\n') % background)
    f.write('Number of order points (max/min evaluation):\t%d\r\n' % args['order'])
    f.write('\r\n')


# Plot the Figure
plt.figure()
for l in xrange(360/15 + 1):
    plt.axvline(x=l*15-120, color="lightgrey")
plt.plot(data[:, 0], data[:, 1])
plt.axis([-60, 60, 0.0, data[:, 1].max() * 1.1])
plt.xticks(numpy.arange(-120/2.0, 120/2.0, 15))
plt.xlabel('angle (deg)')
plt.ylabel('rel. signal')

# Plot background
plt.axhline(y=background, color='grey', linestyle='--')

# Plot max min
plt.plot(data[:, 0][max], data[:, 1][max], 'ro')
plt.plot(data[:, 0][min], data[:, 1][min], 'go')

# Plot section borders
for l in ([data[:, 0][leftidx], data[:, 0][rightidx + max[-1]]] + data[:, 0][min].tolist()):
    plt.axvline(x=l, color='red', linestyle='--')

# Plot FWHM points and lines
for l in xrange(len(FWHMpoints)/2):
    ptsx = [FWHMpoints[l * 2][0], FWHMpoints[l * 2 + 1][0]]
    ptsy = [FWHMpoints[l * 2][1], FWHMpoints[l * 2 + 1][1]]
    plt.plot(ptsx, ptsy, 'bo-')

# Plot fitting functions
for index, sec in enumerate(sections):
    xvalues = numpy.linspace(data[sec[1]:(sec[2] + 1), 0].min(), data[sec[1]:(sec[2] + 1), 0].max(), num=50)
    plt.plot(xvalues, Lorentzian(xvalues, *fitparams[index]), linestyle='-', linewidth=3)

plt.savefig(os.path.splitext(args['infile'])[0]+'.evaluated.png')




